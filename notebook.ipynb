{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project Submission\n",
    "\n",
    "***\n",
    "- Name: Adam Marianacci\n",
    "- Scheduled project review date/time: 3/19/2024/1:30pm EST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is my task to help companies with computer vision for their A.I. tennis ball machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created all the data used in this project. I used roughly 6,000 images that consisted of a balanced class of forehand and backhand tennis shots. \n",
    "The limitations of the dataset was that it was a fairly small set of data. The images used only consisted of myself hitting forehand and backhand tennis shots. The data used for this project was useful to build a convolutional neural network for image classification because their was a minimal amount of variance in the data used. The neural net was able to learn specific patterns about forehands and backhands with minimal \"noise\" in the data because of the consistency of the data used.\n",
    "\n",
    "Dataset:[ForehandsandBackhands](https://www.kaggle.com/datasets/adammarianacci/forehands-and-backhands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   I started by filming myself in a few different locations hitting approximately 200 forehands and 200 backhands. I then used an open source editing program called \"Shotcut\" to edit all of my swings from the start of my swing up until the point of contact with the tennis ball. I then converted these edited video shots into an mp4 format so that they could be extracted into frames. One shot clip yieled roughly 15 images of data.\n",
    "    I extracted the frames into folders and labeled the frames as either belonging to one of two classes \"1\" for forehand and \"0\" for backhand. I constructed an iterator into my pipeline to be able to call on specific \"batches\" of data. I then scaled my data in the pipeline to make sure that all the images used were the same size.\n",
    "    I then set up a train, test, split which was 80% for training, 10% for validation, and 10% for testing. I then set up a way to save and load my data after it has been trained into my pipeline so that more data could be input in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility I uploaded my data to Kaggle and tested my notebook in Google Colab to ensure my notebook would run smoothly. If you wish to access the data and run this notebook on another platform such as Google Colab you will have to have a Kaggle account. \n",
    "- In the cell below you will have to enter your Kaggle username and API Key.\n",
    "This [creating token](https://stackoverflow.com/a/71674371) link will help give you instructions on how to obtain an API key.\n",
    "- This [link](https://www.kaggle.com/docs/api) is also helpful for more information on how to use Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cell below if you wish to work from another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# # please enter your Kaggle username and api key here\n",
    "# username = ''\n",
    "# key = ''\n",
    "\n",
    "# # your api key\n",
    "# api_key = {\n",
    "# 'username': username,\n",
    "# 'key': key}\n",
    "\n",
    "# # uses pathlib Path\n",
    "# kaggle_path = Path('/root/.kaggle')\n",
    "# os.makedirs(kaggle_path, exist_ok=True)\n",
    "\n",
    "# # opens file and dumps python dict to json object \n",
    "# with open (kaggle_path/'kaggle.json', 'w') as handl:\n",
    "#     json.dump(api_key,handl)\n",
    "\n",
    "# os.chmod(kaggle_path/'kaggle.json', 600)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cell below if you would like to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This downloads the images for training and testing\n",
    "# !kaggle datasets download -d adammarianacci/forehands-and-backhands\n",
    "# !unzip forehands-and-backhands.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import imghdr\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below enables you to select videos within a specified folder. The function enables you to extract frames from the video file and puts them into a specified folder within the directory. It iterates thru all video files ending with' mp4' and 'avi' and saves the the extracted frames into a specified folder. If you wish to use this function uncomment the cell beloew and use your own file paths for the 'video_directory' and 'output directory' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory containing the video files\n",
    "# video_directory = '/home/adam/Desktop/video_test'\n",
    "\n",
    "# # Output directory for extracted frames\n",
    "# output_directory = '/home/adam/Desktop/frame_test'\n",
    "\n",
    "# # Function to extract frames from a video file\n",
    "# def extract_frames(video_file, output_dir):\n",
    "#     # Open the video file\n",
    "#     vid = cv2.VideoCapture(video_file)\n",
    "#     if not vid.isOpened():\n",
    "#         print(f\"Error: Couldn't open video file '{video_file}'\")\n",
    "#         return\n",
    "\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Initialize frame counter\n",
    "#     current_frame = 0\n",
    "\n",
    "#     # Read frames and save them as images\n",
    "#     while True:\n",
    "#         success, frame = vid.read()\n",
    "\n",
    "#         if not success:\n",
    "#             print(f\"End of video reached or couldn't read frame from '{video_file}'\")\n",
    "#             break\n",
    "\n",
    "#         if frame.shape[0] > 0 and frame.shape[1] > 0:\n",
    "#             # Save the frame as an image\n",
    "#             output_path = os.path.join(output_dir, f'{os.path.splitext(os.path.basename(video_file))[0]}_{current_frame}.jpg')\n",
    "#             cv2.imwrite(output_path, frame)\n",
    "#             current_frame += 1\n",
    "#         else:\n",
    "#             print(f\"Error: Invalid frame size in '{video_file}'\")\n",
    "\n",
    "#     vid.release()\n",
    "\n",
    "# # Iterate over video files in the directory\n",
    "# for video_file in os.listdir(video_directory):\n",
    "#     if video_file.endswith('.mp4') or video_file.endswith('.avi'):\n",
    "#         video_path = os.path.join(video_directory, video_file)\n",
    "#         extract_frames(video_path, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a data directory\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below sets up a variable to recognize different images types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining image extensions\n",
    "image_exts = ['jpeg', 'jpg', 'bmp', 'png']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows that my data directory contains my 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the contents of my data directory\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below sets up a for loop that goes thru the folders and all images in the data directory and removes any images that does not have the appropriate image extensions. Note the skipping output is just skipping directories but there are no issues with the actual files in the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through directories containing images\n",
    "for image_class in os.listdir(data_dir):\n",
    "    class_dir = os.path.join(data_dir, image_class)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue  # Skip if not a directory\n",
    "    for image in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, image)\n",
    "        if not os.path.isfile(image_path):\n",
    "            print('Skipping non-file:', image_path)\n",
    "            continue  # Skip if not a file\n",
    "        try:\n",
    "            tip = imghdr.what(image_path)  # Get image type directly\n",
    "            if tip is None or tip not in image_exts:\n",
    "                print('Image not in ext list or invalid format: {}'.format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print('Issue with image {}: {}'.format(image_path, str(e)))\n",
    "            # Log the error or handle it appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to preprocess the data, we are starting with approx. 6,000 images belonging to two classes that are balanced. This sets up the data pipleline and enables access to \"batches\" of data using the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading in tensorflow and keras on our data \n",
    "data = tf.keras.preprocessing.image_dataset_from_directory('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow acces to generator from the data pipeline\n",
    "data_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow access to pull a batch (32 images) of data\n",
    "batch = data_iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size of 32, image size of 256x256, 3 (channels of color, RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images represented as numpy arrays\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representing the labels in the batch\n",
    "batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am plotting some images from a batch to see which classes my images belong to and we can see Forehands = 1 and Backhands = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# viewing some  data to see what class my frames belong to\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell transforms the images \"X\" to all be scaled for optimization in the data pipeline. I typed in .map function using lambda in google and found this [solution](https://stackoverflow.com/questions/48234088/mapping-arrays-in-python-using-map-lambda-and-functional-programming-workflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data in the pipeline\n",
    "data = data.map(lambda X,y: (X/255, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The iterator is now scaled from 0 to 1\n",
    "scaled_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the variable batch so that it will always be scaled\n",
    "batch = scaled_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the min is 0\n",
    "batch[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming max is 1\n",
    "batch[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are visualizing another batch of data to make sure everything is working on the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing a batch of data to see what class my frames belong to\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am ready to start my train, test, split. I check the length of the data (in batches) and we see we have 89 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the length of the data in batches\n",
    "len(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I shuffled the data so that when I eventually use it for modeling it is not biased by learning about the images in any particular order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "data_shuffled = data.shuffle(buffer_size=len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splits that data for training, testing, and validation by the number by batches. I designated 80% to training which is 158 batches, 10% testing which is 15 batches, and 10% for validation which is 15 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into 80% training, 10% testing, 10% validation\n",
    "train_size = int(len(data_shuffled)*.8)\n",
    "val_size = int(len(data_shuffled)*.1)+1\n",
    "test_size = int(len(data_shuffled)*.1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the splits equal to 198 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 198 batches\n",
    "train_size+val_size+test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below now sets up the variables to either take or skip certain batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_shuffled.take(train_size)\n",
    "val = data_shuffled.skip(train_size).take(val_size)\n",
    "test = data_shuffled.skip(train_size + val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is a convolutional layer with 16 filters, it is 3 pixels x 3pixels, and a stride of 1 so it goes pixel by pixel. The relu activation function takes into account any non linear patterns. The input shape is the size of our images. The next 2 layers are condensing the rows and width. The last 2 layers are dense fully connected layers and a sigmoid activation function is applied for the output to be between 0 and 1. An output of closer to 1 will be part of the forehand class, and an output of 0 will be closer to the Backhand class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(32, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below passes through an optomizer, and BinaryCrossentropy to our loss because we are dealing with Binary Classification, and the metrics we are going to be evaluating on will be accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows how the model takes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a log directory that is my logs folder\n",
    "logdir = 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a variable 'tensorboard_callback' which allows you to log the model training while it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below sets up the training process at 20 epochs for the baseline model. It will also be evaluating on our validation data as well. The below cell is commented out to save time as it takes around 45 minutes to run. The model was quickly overfitting after a few epochs so to save time I adjusted the model to 3 epochs which is currently our best performing model. This also saves the model as an h5 file in the specified folder 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code will initiate training, it will take approx 45 minutes\n",
    "#hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])\n",
    "# model.save(os.path.join('models', 'forehandbackhandmodel_20.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code will initiate training, it will take approx 15 minutes\n",
    "hist = model.fit(train, epochs=3, validation_data=val, callbacks=[tensorboard_callback])\n",
    "#model.save(os.path.join('models', 'forehandbackhandmodel_3.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the performance which led to 100% accuracy and a decreasing loss close that dropped close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing our loss\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing our accuracy\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have trained on our model on training and validation data and have visualized the loss and metrics of accuracy we can see how it performs on testing images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in a newly trained model to evaluate on testing images\n",
    "#new_model = load_model(os.path.join('models', 'forehandbackhandmodel_3.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating performance on our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = Precision()\n",
    "recall = Recall()\n",
    "accuracy = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator():\n",
    "    X, y = batch\n",
    "    y_hat = model.predict(X)\n",
    "    precision.update_state(y, y_hat)\n",
    "    recall.update_state(y, y_hat)\n",
    "    accuracy.update_state(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our testing data is performing at 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Precision:{precision.result().numpy()}, Recall:{recall.result().numpy()},  Accuracy:{accuracy.result().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a test on an image not in our batch\n",
    "img = cv2.imread('data/backhands/frames/backhandvids_1004.jpg')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = tf.image.resize(img, (256,256))\n",
    "plt.imshow(resize.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on a random image the model has never seen before. This preprocesses an image an image array and feeds it into the NN 'model' and returns a prediction probability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is predicting what our image is \n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is a conditional statement that prints out what the predicted class is. We are taking the probability score of 'y_hat' which is our target class prediction and setting a threshold of 0.5. If the probability score is greather than 0.5 the predicted class will be a forehand and if it is less than 0.5 the predicted class will be a backhand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_hat > 0.5:\n",
    "    print(f'Predddicted class is Forehand')\n",
    "else:\n",
    "    print(f'Predicted class is Backhand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My best performing model was the CNN with an 'Adam' optomizer and loss function set to 'Binary Cross-Entropy' trained over 3 epochs. I adjusted the model from 20 epochs to 3 because the model was quickly acheiving a 100% accuracy score so this approximately saved about 1 hour of training time. The model was showing 100% accuracy scores on training and testing data. The model is most likely overfitting due to the low variance in my data making it over confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model showed scores of 100% on accuracy, precision, and recall. The model is performing perfectly most likely due to the low variance in my dataset leading to it's overconfidence. To build a more robust model 10x more data with high variance needs to be inputted into to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limited amount of data was used. The data consisted only of myself from one camera angle in a few different locations with minimal background \"noise\". Additionally the model only learned about two-handed backhand shots and not one-handed backhand shots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to increase the amount of variance in the dataset. Since the current model is performing so well we can start to add images from the start of swing preparation until the completion of the swing and not just up until the point of contact with the ball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain more 10x more data with more variance of different people of all ages hitting shots from different camera angles in different backgrounds. The real world is \"noisier\" so we need to start training our model this way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env-1]",
   "language": "python",
   "name": "conda-env-learn-env-1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
